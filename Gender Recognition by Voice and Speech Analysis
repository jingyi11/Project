setwd("/Users/gjy/Desktop/6289 data mining/final")
voice.train <- read.csv("data.train.csv")
voice.test <- read.csv("data.test.new.csv")

#split the voice data into 2 groups: train and test
set.seed(2)
traindata=sample(1:nrow(voice.train), 2144)
test=voice.train[-traindata,]
train=voice.train[traindata,]

#classification trees
library(tree)
library(ISLR)
tree.train <- tree(label~.,train)
summary(tree.train)
##train Misclassification error rate: 0.03965 = 85 / 2144  
tree.pred <- predict(tree.train, test, type="class")
label.test <- test[,21]
table(tree.pred, label.test)
##test Misclassification error rate: 0.05037 = 27 / 536 

#tree pruning
set.seed(3)
cv.train <- cv.tree(tree.train, FUN=prune.misclass)
names(cv.train)
cv.train
par(mfrow =c(1,2))
plot(cv.train$size ,cv.train$dev ,type="b")
plot(cv.train$k ,cv.train$dev ,type="b")
#The second tree results in the lowest cross-validation error rate and it is 100/2144=0.0466 and the corresponding size is 5.
prune.train =prune.misclass (tree.train ,best =5)
tree.pred <- predict(prune.train, test, type="class")
table(tree.pred, label.test)
##test Misclassification error rate: 0.05037 = 27 / 536 

#random forest
library (randomForest)
set.seed (1)
bag.voice=randomForest(label~.,data=train,
mtry=10, importance =TRUE)
bag.voice
yhat.bag = predict (bag.voice,newdata =test)
table(yhat.bag,label.test)
#svm
library(e1071)
svmfit <- svm(label~., data=train, kernel="linear", cost=10, scale=FALSE)
summary(svmfit)
set.seed(1)
tune.out=tune(svm,label~.,data=train ,kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5),gamma=c(0.01,0.05,0.1)))
summary (tune.out)
bestmod =tune.out$best.model
summary (bestmod)
svm.pred <- predict(bestmod, test)
table(svm.pred, label.test)
##test Misclassification error rate: 0.0354 = 19 / 536 

#Neural network
library(ISLR)
maxs <- apply(voice.train[,1:20], 2, max)
mins <- apply(voice.train[,1:20], 2, min)
scaled.data <- as.data.frame(scale(voice.train[,1:20],
center = mins, scale = maxs - mins))
label.nn <- as.numeric(voice.train$label)-1
data <- cbind(scaled.data, label.nn)
library(caTools)
set.seed(101)
split = sample.split(data$label.nn, SplitRatio = 0.80)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)
feats <- names(scaled.data)
f <- paste(feats,collapse=' + ')
f <- paste('label.nn~',f)
f <- as.formula(f)
library(neuralnet)
nn <- neuralnet(f,train,hidden=c(10,10,10),linear.output=FALSE)
predicted.nn.values <- compute(nn,test[1:20])
predicted.nn.values$net.result <-sapply(predicted.nn.values$net.result,round,digits=0)
table(test$label.nn,predicted.nn.values$net.result)
##test Misclassification error rate: 0.0261 = 23 / 536 

#The test result is best by using svm.
svm.predfinal <- predict(bestmod, voice.test)
write.table(svm.predfinal, "/Users/gjy/Desktop/6289 data mining/final/output.txt", row.names = FALSE,col.names = FALSE, sep="\t")
